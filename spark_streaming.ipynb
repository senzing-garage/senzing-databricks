{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54742490",
   "metadata": {},
   "source": [
    "# Spark Streaming and Senzing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85510a2a",
   "metadata": {},
   "source": [
    "This notebook shows you how to process streaming data through Apache Spark and send it to Senzing for entity resolution, simulating a real-time data processing pipeline. If you haven't already gone through the `senzing_quickstart.ipynb` tutorial in this repository, we recommend starting with that one because it contains more detailed explanations for each of the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70eff95",
   "metadata": {},
   "source": [
    "### Steps in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459273ad",
   "metadata": {},
   "source": [
    "1. Set up the Senzing gRPC server, download the `customer.json` data file and split it into 20 separate JSONL files to simulate streaming data.\n",
    "2. Configure the Senzing engine so it's ready to receive data.\n",
    "3. Create a Spark session with streaming capabilities, define a schema and set up a streaming dataframe.\n",
    "4. Implement a batch processing function that takes each streaming batch from Spark, sends individual records to Senzing for entity resolution, and tracks which entities are affected by each record addition.\n",
    "5. Run a cleanup process to ensure the entities are as accurate as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f32414",
   "metadata": {},
   "source": [
    "## Set up requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a0d89",
   "metadata": {},
   "source": [
    "In this tutorial, we'll use the [`senzing`](https://garage.senzing.com/sz-sdk-python/index.html) and [`senzing_grpc`](https://garage.senzing.com/sz-sdk-python-grpc/) packages, in addition to PySpark. Install these using the `requirements.txt` file in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ac5b42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:24.897956Z",
     "iopub.status.busy": "2025-10-17T22:01:24.897790Z",
     "iopub.status.idle": "2025-10-17T22:01:24.996873Z",
     "shell.execute_reply": "2025-10-17T22:01:24.996430Z",
     "shell.execute_reply.started": "2025-10-17T22:01:24.897941Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.streaming import StreamingQuery\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from senzing import szengineflags, szerror\n",
    "from senzing_grpc import SzAbstractFactoryGrpc, SzConfigManagerGrpc, SzDiagnosticGrpc, SzEngineGrpc, SzConfigGrpc, SzProductGrpc  # type: ignore\n",
    "import grpc\n",
    "import requests\n",
    "import watermark   # type: ignore  # pylint: disable=W0611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6a421d-95df-4948-8843-551dd2e99767",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:24.997263Z",
     "iopub.status.busy": "2025-10-17T22:01:24.997154Z",
     "iopub.status.idle": "2025-10-17T22:01:25.010025Z",
     "shell.execute_reply": "2025-10-17T22:01:25.009553Z",
     "shell.execute_reply.started": "2025-10-17T22:01:24.997252Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2025-10-17T15:01:24.997997-07:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.13.8\n",
      "IPython version      : 9.6.0\n",
      "\n",
      "Compiler    : Clang 17.0.0 (clang-1700.0.13.3)\n",
      "OS          : Darwin\n",
      "Release     : 24.6.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 14\n",
      "Architecture: 64bit\n",
      "\n",
      "json        : 2.0.9\n",
      "senzing     : 4.0.4\n",
      "senzing_grpc: 0.5.13\n",
      "watermark   : 2.5.0\n",
      "grpc        : 1.75.1\n",
      "pyspark     : 4.0.1\n",
      "requests    : 2.32.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f36227",
   "metadata": {},
   "source": [
    "We'll start our [Senzing gRPC server](https://github.com/senzing-garage/serve-grpc/tree/main) using Docker.\n",
    "\n",
    "Run the following command in a terminal window:\n",
    "\n",
    "```bash\n",
    "docker run -it --publish 8261:8261 --rm senzing/serve-grpc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40530f4a",
   "metadata": {},
   "source": [
    "Then download the example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b64cb50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:25.010674Z",
     "iopub.status.busy": "2025-10-17T22:01:25.010547Z",
     "iopub.status.idle": "2025-10-17T22:01:25.012893Z",
     "shell.execute_reply": "2025-10-17T22:01:25.012289Z",
     "shell.execute_reply.started": "2025-10-17T22:01:25.010664Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH: str = \"./data/\"\n",
    "\n",
    "DATA_URL_PREFIX: str = \"https://raw.githubusercontent.com/Senzing/truth-sets/refs/heads/main/truthsets/demo/\"\n",
    "\n",
    "FILENAME: str = \"reference.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "753c0f18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:25.013292Z",
     "iopub.status.busy": "2025-10-17T22:01:25.013194Z",
     "iopub.status.idle": "2025-10-17T22:01:25.015453Z",
     "shell.execute_reply": "2025-10-17T22:01:25.015040Z",
     "shell.execute_reply.started": "2025-10-17T22:01:25.013281Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(DATA_PATH, exist_ok = True)\n",
    "\n",
    "URL: str = DATA_URL_PREFIX + FILENAME\n",
    "FILEPATH: str = DATA_PATH + FILENAME\n",
    "\n",
    "if not os.path.exists(FILEPATH):\n",
    "    response: requests.Response = requests.get(URL, stream = True, timeout = 10)\n",
    "    response.raw.decode_content = True\n",
    "\n",
    "    with open(FILEPATH, \"wb\") as file:\n",
    "        shutil.copyfileobj(response.raw, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b952a68e",
   "metadata": {},
   "source": [
    "## Create separate JSONL files to simulate streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ae673",
   "metadata": {},
   "source": [
    "We'll use the `reference.json` file from the Senzing \"Truthsets\" <https://github.com/Senzing/truth-sets/> demo data. This dataset contains customer and organization information, with incomplete contact data.\n",
    "\n",
    "We'll save each record from the `reference.json` file into a separate JSONL file to simulate streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa194d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:25.015923Z",
     "iopub.status.busy": "2025-10-17T22:01:25.015836Z",
     "iopub.status.idle": "2025-10-17T22:01:25.018440Z",
     "shell.execute_reply": "2025-10-17T22:01:25.018037Z",
     "shell.execute_reply.started": "2025-10-17T22:01:25.015915Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_streaming_files (\n",
    "        input_file: str,\n",
    "        output_dir: str,\n",
    "    ) -> None:\n",
    "    \"\"\"simulate streaming data\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "    with open(input_file, \"r\", encoding = \"utf-8\") as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            try:\n",
    "                record: dict = json.loads(line)\n",
    "                filename: str = f\"{output_dir}/record_{record['RECORD_ID']}.json\"\n",
    "\n",
    "                with open(filename, \"w\", encoding = \"utf-8\") as out_file:\n",
    "                    json.dump(record, out_file)\n",
    "\n",
    "                print(f\"Created {filename}\")\n",
    "\n",
    "            except json.JSONDecodeError as ex:\n",
    "                print(f\"Error parsing line {i}: {ex}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f0db36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:25.018867Z",
     "iopub.status.busy": "2025-10-17T22:01:25.018736Z",
     "iopub.status.idle": "2025-10-17T22:01:25.022640Z",
     "shell.execute_reply": "2025-10-17T22:01:25.022192Z",
     "shell.execute_reply.started": "2025-10-17T22:01:25.018858Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data/streaming/record_2012.json\n",
      "Created data/streaming/record_2013.json\n",
      "Created data/streaming/record_2014.json\n",
      "Created data/streaming/record_2041.json\n",
      "Created data/streaming/record_2051.json\n",
      "Created data/streaming/record_2061.json\n",
      "Created data/streaming/record_2071.json\n",
      "Created data/streaming/record_2074.json\n",
      "Created data/streaming/record_2081.json\n",
      "Created data/streaming/record_2091.json\n",
      "Created data/streaming/record_2101.json\n",
      "Created data/streaming/record_2102.json\n",
      "Created data/streaming/record_2111.json\n",
      "Created data/streaming/record_2112.json\n",
      "Created data/streaming/record_2121.json\n",
      "Created data/streaming/record_2122.json\n",
      "Created data/streaming/record_2131.json\n",
      "Created data/streaming/record_2132.json\n",
      "Created data/streaming/record_2141.json\n",
      "Created data/streaming/record_2151.json\n",
      "Created data/streaming/record_2161.json\n",
      "Created data/streaming/record_2162.json\n"
     ]
    }
   ],
   "source": [
    "create_streaming_files(\n",
    "    \"data/reference.json\",\n",
    "    \"data/streaming\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e185bd",
   "metadata": {},
   "source": [
    "## Configure Senzing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec216b",
   "metadata": {},
   "source": [
    "Next, configure the Senzing engine to accept the `reference` dataset, in the same way as the `spark_quickstart.ipynb` tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f446a55e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:25.023023Z",
     "iopub.status.busy": "2025-10-17T22:01:25.022939Z",
     "iopub.status.idle": "2025-10-17T22:01:25.029329Z",
     "shell.execute_reply": "2025-10-17T22:01:25.028858Z",
     "shell.execute_reply.started": "2025-10-17T22:01:25.023016Z"
    }
   },
   "outputs": [],
   "source": [
    "grpc_channel: grpc.Channel = grpc.insecure_channel(\"localhost:8261\")\n",
    "sz_abstract_factory: SzAbstractFactoryGrpc = SzAbstractFactoryGrpc(grpc_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d822b",
   "metadata": {},
   "source": [
    "Check connectivity by getting the Senzing version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ec6a775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:25.029708Z",
     "iopub.status.busy": "2025-10-17T22:01:25.029627Z",
     "iopub.status.idle": "2025-10-17T22:01:25.041467Z",
     "shell.execute_reply": "2025-10-17T22:01:25.041043Z",
     "shell.execute_reply.started": "2025-10-17T22:01:25.029700Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"PRODUCT_NAME\": \"Senzing SDK\",\n",
      "  \"VERSION\": \"4.0.0\",\n",
      "  \"BUILD_VERSION\": \"4.0.0.25184\",\n",
      "  \"BUILD_DATE\": \"2025-07-03\",\n",
      "  \"BUILD_NUMBER\": \"2025_07_03__16_38\",\n",
      "  \"COMPATIBILITY_VERSION\": {\n",
      "    \"CONFIG_VERSION\": \"11\"\n",
      "  },\n",
      "  \"SCHEMA_VERSION\": {\n",
      "    \"ENGINE_SCHEMA_VERSION\": \"4.0\",\n",
      "    \"MINIMUM_REQUIRED_SCHEMA_VERSION\": \"4.0\",\n",
      "    \"MAXIMUM_REQUIRED_SCHEMA_VERSION\": \"4.99\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "sz_product: SzProductGrpc = sz_abstract_factory.create_product()\n",
    "version_json: str = json.loads(sz_product.get_version())\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        version_json,\n",
    "        indent = 2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96d8e7a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:25.041934Z",
     "iopub.status.busy": "2025-10-17T22:01:25.041847Z",
     "iopub.status.idle": "2025-10-17T22:01:25.044123Z",
     "shell.execute_reply": "2025-10-17T22:01:25.043647Z",
     "shell.execute_reply.started": "2025-10-17T22:01:25.041925Z"
    }
   },
   "outputs": [],
   "source": [
    "sz_configmanager: SzConfigManagerGrpc = sz_abstract_factory.create_configmanager()\n",
    "sz_diagnostic: SzDiagnosticGrpc = sz_abstract_factory.create_diagnostic()\n",
    "sz_engine: SzEngineGrpc = sz_abstract_factory.create_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7324336c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:25.044463Z",
     "iopub.status.busy": "2025-10-17T22:01:25.044369Z",
     "iopub.status.idle": "2025-10-17T22:01:25.052484Z",
     "shell.execute_reply": "2025-10-17T22:01:25.052030Z",
     "shell.execute_reply.started": "2025-10-17T22:01:25.044455Z"
    }
   },
   "outputs": [],
   "source": [
    "config_id: int = sz_configmanager.get_default_config_id()\n",
    "sz_config:SzConfigGrpc = sz_configmanager.create_config_from_config_id(config_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e64881f",
   "metadata": {},
   "source": [
    "This time, we'll only use a single data source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a23e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:25.052793Z",
     "iopub.status.busy": "2025-10-17T22:01:25.052721Z",
     "iopub.status.idle": "2025-10-17T22:01:25.058228Z",
     "shell.execute_reply": "2025-10-17T22:01:25.057729Z",
     "shell.execute_reply.started": "2025-10-17T22:01:25.052783Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sz_config.register_data_source(\"REFERENCE\")\n",
    "\n",
    "except (grpc.RpcError, szerror.SzError) as err:\n",
    "    print(err, \"\\n\")\n",
    "    print(\"You only need to register the data source once\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e23073d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:25.058645Z",
     "iopub.status.busy": "2025-10-17T22:01:25.058566Z",
     "iopub.status.idle": "2025-10-17T22:01:25.070396Z",
     "shell.execute_reply": "2025-10-17T22:01:25.070004Z",
     "shell.execute_reply.started": "2025-10-17T22:01:25.058638Z"
    }
   },
   "outputs": [],
   "source": [
    "new_json_config: str = sz_config.export()\n",
    "\n",
    "new_config_id: int = sz_configmanager.register_config(\n",
    "    new_json_config,\n",
    "    \"Spark Streaming\",\n",
    ")\n",
    "\n",
    "sz_configmanager.replace_default_config_id(\n",
    "    config_id,\n",
    "    new_config_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b54b6bd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:25.070771Z",
     "iopub.status.busy": "2025-10-17T22:01:25.070698Z",
     "iopub.status.idle": "2025-10-17T22:01:25.515540Z",
     "shell.execute_reply": "2025-10-17T22:01:25.513678Z",
     "shell.execute_reply.started": "2025-10-17T22:01:25.070764Z"
    }
   },
   "outputs": [],
   "source": [
    "sz_abstract_factory.reinitialize(new_config_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca643f26",
   "metadata": {},
   "source": [
    "## Set up the Spark Streaming functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dccf1b",
   "metadata": {},
   "source": [
    "Start a new Spark session, create a schema, and then set up a stream reader from Spark's [Structured Streaming](https://spark.apache.org/docs/latest/streaming/index.html) engine. \n",
    "\n",
    "In the next section, we'll use a stream writer to send the data from the Spark Streaming dataframe to Senzing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bddc7b",
   "metadata": {},
   "source": [
    "First, create a Spark session. Ignore any warnings about `NativeCodeLoader` or `jdk.incubator.vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81a8d554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:25.516658Z",
     "iopub.status.busy": "2025-10-17T22:01:25.516476Z",
     "iopub.status.idle": "2025-10-17T22:01:27.568714Z",
     "shell.execute_reply": "2025-10-17T22:01:27.568225Z",
     "shell.execute_reply.started": "2025-10-17T22:01:25.516642Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/17 15:01:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Senzing Streaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoint\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e861b",
   "metadata": {},
   "source": [
    "Providing a schema for our data makes sure that all the files have the correct information.\n",
    "This also speeds up the Spark stream reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a037025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:27.569105Z",
     "iopub.status.busy": "2025-10-17T22:01:27.569028Z",
     "iopub.status.idle": "2025-10-17T22:01:27.572275Z",
     "shell.execute_reply": "2025-10-17T22:01:27.571876Z",
     "shell.execute_reply.started": "2025-10-17T22:01:27.569099Z"
    }
   },
   "outputs": [],
   "source": [
    "customers_schema = StructType([\n",
    "    StructField(\"DATA_SOURCE\", StringType(), True),\n",
    "    StructField(\"RECORD_ID\", StringType(), True),\n",
    "    StructField(\"RECORD_TYPE\", StringType(), True),\n",
    "    StructField(\"PRIMARY_NAME_ORG\", StringType(), True),\n",
    "    StructField(\"SECONDARY_NAME_ORG\", StringType(), True),\n",
    "    StructField(\"PRIMARY_NAME_FULL\", StringType(), True),\n",
    "    StructField(\"NATIVE_NAME_FULL\", StringType(), True),\n",
    "    StructField(\"PRIMARY_NAME_LAST\", StringType(), True),\n",
    "    StructField(\"PRIMARY_NAME_FIRST\", StringType(), True),\n",
    "    StructField(\"PRIMARY_NAME_MIDDLE\", StringType(), True),\n",
    "    StructField(\"GENDER\", StringType(), True),\n",
    "    StructField(\"DATE_OF_BIRTH\", StringType(), True),\n",
    "    StructField(\"PASSPORT_NUMBER\", StringType(), True),\n",
    "    StructField(\"PASSPORT_COUNTRY\", StringType(), True),\n",
    "    StructField(\"DRIVERS_LICENSE_NUMBER\", StringType(), True),\n",
    "    StructField(\"DRIVERS_LICENSE_STATE\", StringType(), True),\n",
    "    StructField(\"SSN_NUMBER\", StringType(), True),\n",
    "    StructField(\"NATIONAL_ID_NUMBER\", StringType(), True),\n",
    "    StructField(\"NATIONAL_ID_COUNTRY\", StringType(), True),\n",
    "    StructField(\"ADDR_TYPE\", StringType(), True),\n",
    "    StructField(\"ADDR_FULL\", StringType(), True),\n",
    "    StructField(\"ADDR_LINE1\", StringType(), True),\n",
    "    StructField(\"ADDR_CITY\", StringType(), True),\n",
    "    StructField(\"ADDR_STATE\", StringType(), True),\n",
    "    StructField(\"ADDR_POSTAL_CODE\", StringType(), True),\n",
    "    StructField(\"ADDR_COUNTRY\", StringType(), True),\n",
    "    StructField(\"PHONE_TYPE\", StringType(), True),\n",
    "    StructField(\"PHONE_NUMBER\", StringType(), True),\n",
    "    StructField(\"EMAIL_ADDRESS\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"STATUS\", StringType(), True),\n",
    "    StructField(\"CATEGORY\", StringType(), True),\n",
    "    StructField(\"AMOUNT\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d24be2",
   "metadata": {},
   "source": [
    "The stream reader uses this schema to write to a streaming dataframe. For this example, it reads one file at a time to simulate streaming, but you can easily change this to your input stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2a1c9f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:27.572630Z",
     "iopub.status.busy": "2025-10-17T22:01:27.572561Z",
     "iopub.status.idle": "2025-10-17T22:01:28.015288Z",
     "shell.execute_reply": "2025-10-17T22:01:28.014742Z",
     "shell.execute_reply.started": "2025-10-17T22:01:27.572623Z"
    }
   },
   "outputs": [],
   "source": [
    "streaming_df: DataFrame = spark \\\n",
    "    .readStream \\\n",
    "    .schema(customers_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1)  \\\n",
    "    .json(\"data/streaming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496e7a7",
   "metadata": {},
   "source": [
    "## Add records to Senzing and to the Spark Streaming dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55026b",
   "metadata": {},
   "source": [
    "Use the `get_affected_entities` function from the `spark_quickstart` tutorial to track what entities have been changed or created in the Senzing repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b97202b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:28.015753Z",
     "iopub.status.busy": "2025-10-17T22:01:28.015664Z",
     "iopub.status.idle": "2025-10-17T22:01:28.017574Z",
     "shell.execute_reply": "2025-10-17T22:01:28.017167Z",
     "shell.execute_reply.started": "2025-10-17T22:01:28.015743Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_affected_entities (\n",
    "    rec_info: str,  # pylint: disable=W0621\n",
    "    ) -> list:\n",
    "    \"\"\"helper function to extract the `ENTITY_ID`\"\"\"\n",
    "    info: list = json.loads(rec_info)\n",
    "\n",
    "    return [ entity[\"ENTITY_ID\"] for entity in info[\"AFFECTED_ENTITIES\"] ]  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08b512e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:28.017919Z",
     "iopub.status.busy": "2025-10-17T22:01:28.017842Z",
     "iopub.status.idle": "2025-10-17T22:01:28.019426Z",
     "shell.execute_reply": "2025-10-17T22:01:28.019060Z",
     "shell.execute_reply.started": "2025-10-17T22:01:28.017911Z"
    }
   },
   "outputs": [],
   "source": [
    "affected_entities: set = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab17a0",
   "metadata": {},
   "source": [
    "And we'll use the code from the `spark_quickstart.ipynb` tutorial to create a function that will send a streaming batch to the Senzing engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "531c8d9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:28.019676Z",
     "iopub.status.busy": "2025-10-17T22:01:28.019619Z",
     "iopub.status.idle": "2025-10-17T22:01:28.022177Z",
     "shell.execute_reply": "2025-10-17T22:01:28.021555Z",
     "shell.execute_reply.started": "2025-10-17T22:01:28.019670Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_streaming_batch (\n",
    "    batch_df: DataFrame,\n",
    "    batch_id: int,\n",
    "    ) -> None:\n",
    "    \"\"\"send a streaming batch to Senzing\"\"\"\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "\n",
    "    print(f\"Processing batch {batch_id} with {batch_df.count()} records\")\n",
    "\n",
    "    for row in batch_df.rdd.toLocalIterator():\n",
    "        record: dict = {\n",
    "            k: v\n",
    "            for k, v in row.asDict().items()\n",
    "            if v is not None\n",
    "        }\n",
    "\n",
    "        rec_info: str = sz_engine.add_record(  # pylint: disable=W0621\n",
    "            record[\"DATA_SOURCE\"],\n",
    "            record[\"RECORD_ID\"],\n",
    "            record,\n",
    "            szengineflags.SzEngineFlags.SZ_WITH_INFO,\n",
    "        )\n",
    "\n",
    "        affected_entities.update(get_affected_entities(rec_info))\n",
    "        print(f\"Added record {record['RECORD_ID']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d2506",
   "metadata": {},
   "source": [
    "Then, we'll stream the data from the Spark dataframe to Senzing using a Spark stream writer. Ignore any warnings about `ResolveWriteToStream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6473d93f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:28.022559Z",
     "iopub.status.busy": "2025-10-17T22:01:28.022488Z",
     "iopub.status.idle": "2025-10-17T22:01:28.213624Z",
     "shell.execute_reply": "2025-10-17T22:01:28.213116Z",
     "shell.execute_reply.started": "2025-10-17T22:01:28.022553Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/17 15:01:28 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "streaming_query: StreamingQuery = streaming_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_streaming_batch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8c50c",
   "metadata": {},
   "source": [
    "While this is running, we can get a count of the number of records that have been added to the Senzing repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29fc3f3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:28.213997Z",
     "iopub.status.busy": "2025-10-17T22:01:28.213915Z",
     "iopub.status.idle": "2025-10-17T22:01:28.218685Z",
     "shell.execute_reply": "2025-10-17T22:01:28.218201Z",
     "shell.execute_reply.started": "2025-10-17T22:01:28.213990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "result: dict = json.loads(sz_engine.get_stats())\n",
    "print(result[\"workload\"][\"loadedRecords\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b65c3f",
   "metadata": {},
   "source": [
    "We can view the `affected_entities` set to confirm that entities have been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1342750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:28.219096Z",
     "iopub.status.busy": "2025-10-17T22:01:28.219017Z",
     "iopub.status.idle": "2025-10-17T22:01:28.222655Z",
     "shell.execute_reply": "2025-10-17T22:01:28.222110Z",
     "shell.execute_reply.started": "2025-10-17T22:01:28.219086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affected_entities  # pylint: disable=W0104"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631141a",
   "metadata": {},
   "source": [
    "And, for each of the entities that have been updated, we can pull the details of that entity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c55b3e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:28.223049Z",
     "iopub.status.busy": "2025-10-17T22:01:28.222984Z",
     "iopub.status.idle": "2025-10-17T22:01:28.225134Z",
     "shell.execute_reply": "2025-10-17T22:01:28.224593Z",
     "shell.execute_reply.started": "2025-10-17T22:01:28.223042Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for entity_id in affected_entities:\n",
    "    result = json.loads(sz_engine.get_entity_by_entity_id(entity_id))\n",
    "    print(f\"Entity ID: {str(entity_id)}, Name: {result['RESOLVED_ENTITY']['ENTITY_NAME']}, Record Type: {result['RESOLVED_ENTITY']['FEATURES']['RECORD_TYPE'][0]['FEAT_DESC']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95c004",
   "metadata": {},
   "source": [
    "If you want to scale this up, you can use the `ThreadPoolExecutor` from the Python [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html) module, as shown in this example: <https://github.com/brianmacy/sz_incremental_withinfo-v3/blob/main/sz_incremental_withinfo.py>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a6667",
   "metadata": {},
   "source": [
    "## Process REDO records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37aa0c8",
   "metadata": {},
   "source": [
    "As in the `spark_quickstart.ipynb` tutorial, we'll run the Senzing [redo process](https://senzing.zendesk.com/hc/en-us/articles/360007475133-Processing-REDO) to clean up the entities in the Senzing repository, updating the `affected_entities` set as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d95d8aec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:28.225686Z",
     "iopub.status.busy": "2025-10-17T22:01:28.225592Z",
     "iopub.status.idle": "2025-10-17T22:01:28.229580Z",
     "shell.execute_reply": "2025-10-17T22:01:28.229039Z",
     "shell.execute_reply.started": "2025-10-17T22:01:28.225679Z"
    }
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    redo_record: str = sz_engine.get_redo_record()\n",
    "\n",
    "    if not redo_record:\n",
    "        break\n",
    "\n",
    "    rec_info: str = sz_engine.process_redo_record(\n",
    "        redo_record,\n",
    "        flags = szengineflags.SzEngineFlags.SZ_WITH_INFO,\n",
    "    )\n",
    "\n",
    "    affected_entities.update(get_affected_entities(rec_info))\n",
    "    print(rec_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ab3fd",
   "metadata": {},
   "source": [
    "## Look up the resolved entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c125da2",
   "metadata": {},
   "source": [
    "Now that the entities have been resolved, we can search the Senzing repository for anyone we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f0ea954",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:28.230001Z",
     "iopub.status.busy": "2025-10-17T22:01:28.229917Z",
     "iopub.status.idle": "2025-10-17T22:01:28.236288Z",
     "shell.execute_reply": "2025-10-17T22:01:28.235746Z",
     "shell.execute_reply.started": "2025-10-17T22:01:28.229993Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"RESOLVED_ENTITIES\": [],\n",
      "  \"SEARCH_STATISTICS\": [\n",
      "    {\n",
      "      \"CANDIDATE_KEYS\": {\n",
      "        \"FEATURE_TYPES\": [\n",
      "          {\n",
      "            \"FTYPE_CODE\": \"NAME_KEY\",\n",
      "            \"FOUND\": 0,\n",
      "            \"NOT_FOUND\": 1,\n",
      "            \"GENERIC\": 0\n",
      "          }\n",
      "        ],\n",
      "        \"SUMMARY\": {\n",
      "          \"FOUND\": 0,\n",
      "          \"NOT_FOUND\": 1,\n",
      "          \"GENERIC\": 0\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "search_query: dict = {\n",
    "    \"name_full\": \"Wang Jie\",\n",
    "}\n",
    "\n",
    "search_result: str = sz_engine.search_by_attributes(json.dumps(search_query))  # pylint: disable=C0103\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        json.loads(search_result),\n",
    "        indent = 2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc127d1",
   "metadata": {},
   "source": [
    "You can take a look at the `spark_quickstart` tutorial for details on how to extract data from the Senzing repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e6fd08f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T22:01:28.236706Z",
     "iopub.status.busy": "2025-10-17T22:01:28.236614Z",
     "iopub.status.idle": "2025-10-17T22:01:28.324424Z",
     "shell.execute_reply": "2025-10-17T22:01:28.323787Z",
     "shell.execute_reply.started": "2025-10-17T22:01:28.236698Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except Exception as ex:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5b9217-23c3-4ad0-bef0-6a96ae63b2de",
   "metadata": {},
   "source": [
    "Ignore any `MicroBatchExecution` errors, though if you want to avoid these see\n",
    "<https://www.waitingforcode.com/apache-spark-structured-streaming/stopping-structured-streaming-query/read>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f4eaed",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20793c",
   "metadata": {},
   "source": [
    "If you’re interested in exploring Senzing further, check out the following links:\n",
    "\n",
    "- [Senzing + Docker quickstart](https://senzing.com/docs/quickstart/quickstart_docker/)\n",
    "\n",
    "- [Senzing Learning Portal](https://senzing.com/senzing-learning-portal-signup)\n",
    "\n",
    "- [Senzing SDK Documentation](https://senzing.com/docs/)\n",
    "\n",
    "- [Entity Centric Learning](https://senzing.com/entity-centric-learning-explained/)\n",
    "\n",
    "- [CORD: Collections Of Relatable Data](https://senzing.com/senzing-ready-data-collections-cord)\n",
    "\n",
    "- [Senzing GitHub public repos](https://github.com/senzing-garage)\n",
    "\n",
    "- [\"Graph Power Hour!\" podcast](https://senzing.com/graph-power-hour)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
